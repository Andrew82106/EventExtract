# -*- coding: utf-8 -*-
"""
åºåˆ—æ€§èƒ½è¯Šæ–­å·¥å…·
å¸®åŠ©åˆ†æä¸ºä»€ä¹ˆé•¿åº¦3åºåˆ—çš„F1è¿™ä¹ˆä½
"""
import os
import json
import networkx as nx
from collections import Counter, defaultdict

def load_graph_from_json(file_path):
    """åŠ è½½JSONæ ¼å¼çš„å›¾"""
    with open(file_path, 'r', encoding='utf-8') as f:
        graph_data = json.load(f)
    
    G = nx.DiGraph()
    for node in graph_data.get('nodes', []):
        node_id = node.get('id', '')
        G.add_node(
            node_id,
            event_type=node.get('event_type', ''),
            event_subtype=node.get('event_subtype', ''),
            event_sub_subtype=node.get('event_sub_subtype', '')
        )
    
    for edge in graph_data.get('edges', []):
        source = edge.get('source', '')
        target = edge.get('target', '')
        if source and target:
            G.add_edge(source, target)
    
    return G

def analyze_graph_connectivity(graph):
    """åˆ†æå›¾çš„è¿é€šæ€§"""
    stats = {
        'nodes': graph.number_of_nodes(),
        'edges': graph.number_of_edges(),
        'isolated_nodes': len(list(nx.isolates(graph))),
        'avg_out_degree': sum(dict(graph.out_degree()).values()) / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0,
        'max_path_length': 0,
        'num_3hop_paths': 0
    }
    
    # ç»Ÿè®¡3è·³è·¯å¾„æ•°é‡
    three_hop_paths = 0
    for node in graph.nodes():
        for succ1 in graph.successors(node):
            for succ2 in graph.successors(succ1):
                three_hop_paths += 1
    
    stats['num_3hop_paths'] = three_hop_paths
    
    # è®¡ç®—æœ€é•¿è·¯å¾„
    if graph.number_of_nodes() > 0:
        try:
            # å°è¯•æ‰¾åˆ°æœ€é•¿çš„ç®€å•è·¯å¾„
            max_len = 0
            for node in graph.nodes():
                lengths = nx.single_source_shortest_path_length(graph, node)
                if lengths:
                    max_len = max(max_len, max(lengths.values()))
            stats['max_path_length'] = max_len
        except:
            pass
    
    return stats

def diagnose_graphs_directory(graphs_dir):
    """è¯Šæ–­æ•´ä¸ªç›®å½•çš„å›¾"""
    print("=" * 80)
    print(f"è¯Šæ–­ç›®å½•: {graphs_dir}")
    print("=" * 80)
    
    graph_files = []
    for filename in os.listdir(graphs_dir):
        if filename.startswith('graph_') and filename.endswith('.json') and 'merged' not in filename:
            graph_files.append(os.path.join(graphs_dir, filename))
    
    graph_files.sort()
    print(f"\næ‰¾åˆ° {len(graph_files)} ä¸ªå›¾æ–‡ä»¶\n")
    
    all_stats = []
    
    # ç»Ÿè®¡å„ç±»å›¾çš„åˆ†å¸ƒ
    graphs_by_category = {
        'ç©ºå›¾(0èŠ‚ç‚¹)': 0,
        'å•èŠ‚ç‚¹å›¾(1èŠ‚ç‚¹,0è¾¹)': 0,
        'ç¨€ç–å›¾(æœ‰èŠ‚ç‚¹ä½†æ— 3è·³è·¯å¾„)': 0,
        'æ­£å¸¸å›¾(æœ‰3è·³è·¯å¾„)': 0
    }
    
    three_hop_distribution = Counter()
    
    for i, file_path in enumerate(graph_files[:20], 1):  # åªçœ‹å‰20ä¸ª
        graph = load_graph_from_json(file_path)
        stats = analyze_graph_connectivity(graph)
        all_stats.append(stats)
        
        # åˆ†ç±»
        if stats['nodes'] == 0:
            graphs_by_category['ç©ºå›¾(0èŠ‚ç‚¹)'] += 1
        elif stats['nodes'] == 1:
            graphs_by_category['å•èŠ‚ç‚¹å›¾(1èŠ‚ç‚¹,0è¾¹)'] += 1
        elif stats['num_3hop_paths'] == 0:
            graphs_by_category['ç¨€ç–å›¾(æœ‰èŠ‚ç‚¹ä½†æ— 3è·³è·¯å¾„)'] += 1
        else:
            graphs_by_category['æ­£å¸¸å›¾(æœ‰3è·³è·¯å¾„)'] += 1
        
        three_hop_distribution[stats['num_3hop_paths']] += 1
        
        # æ‰“å°è¯¦æƒ…
        if i <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"[{i}] {os.path.basename(file_path)}")
            print(f"  èŠ‚ç‚¹æ•°: {stats['nodes']}, è¾¹æ•°: {stats['edges']}")
            print(f"  å­¤ç«‹èŠ‚ç‚¹: {stats['isolated_nodes']}")
            print(f"  å¹³å‡å‡ºåº¦: {stats['avg_out_degree']:.2f}")
            print(f"  æœ€é•¿è·¯å¾„: {stats['max_path_length']}")
            print(f"  3è·³è·¯å¾„æ•°: {stats['num_3hop_paths']}")
            if stats['num_3hop_paths'] == 0:
                print(f"  âš ï¸  è­¦å‘Š: æ— æ³•å½¢æˆé•¿åº¦3çš„åºåˆ—!")
            print()
    
    # æ€»ä½“ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("æ€»ä½“ç»Ÿè®¡")
    print("=" * 80)
    
    print("\nå›¾çš„åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in graphs_by_category.items():
        percentage = (count / len(all_stats) * 100) if all_stats else 0
        print(f"  {category}: {count} ({percentage:.1f}%)")
    
    print("\n3è·³è·¯å¾„æ•°é‡åˆ†å¸ƒ:")
    for num_paths, count in sorted(three_hop_distribution.items())[:10]:
        percentage = (count / len(all_stats) * 100) if all_stats else 0
        print(f"  {num_paths}ä¸ª3è·³è·¯å¾„: {count}ä¸ªå›¾ ({percentage:.1f}%)")
    
    # å¹³å‡ç»Ÿè®¡
    if all_stats:
        avg_nodes = sum(s['nodes'] for s in all_stats) / len(all_stats)
        avg_edges = sum(s['edges'] for s in all_stats) / len(all_stats)
        avg_3hop = sum(s['num_3hop_paths'] for s in all_stats) / len(all_stats)
        
        print(f"\nå¹³å‡ç»Ÿè®¡:")
        print(f"  å¹³å‡èŠ‚ç‚¹æ•°: {avg_nodes:.2f}")
        print(f"  å¹³å‡è¾¹æ•°: {avg_edges:.2f}")
        print(f"  å¹³å‡3è·³è·¯å¾„æ•°: {avg_3hop:.2f}")
        
        print(f"\nğŸ” å…³é”®å‘ç°:")
        if avg_3hop < 5:
            print(f"  âŒ å¹³å‡æ¯ä¸ªå›¾åªæœ‰ {avg_3hop:.2f} ä¸ª3è·³è·¯å¾„ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†åºåˆ—åŒ¹é…!")
            print(f"     å»ºè®®: å¢åŠ æ›´å¤šçš„è¾¹è¿æ¥ï¼Œæé«˜å›¾çš„è¿é€šæ€§")
        
        no_3hop_percentage = (graphs_by_category['ç¨€ç–å›¾(æœ‰èŠ‚ç‚¹ä½†æ— 3è·³è·¯å¾„)'] + 
                              graphs_by_category['å•èŠ‚ç‚¹å›¾(1èŠ‚ç‚¹,0è¾¹)'] + 
                              graphs_by_category['ç©ºå›¾(0èŠ‚ç‚¹)']) / len(all_stats) * 100
        
        if no_3hop_percentage > 30:
            print(f"  âŒ {no_3hop_percentage:.1f}% çš„å›¾æ— æ³•å½¢æˆ3è·³è·¯å¾„!")
            print(f"     è¿™æ˜¯å¯¼è‡´Recallä½çš„ä¸»è¦åŸå› ")

def analyze_ground_truth(gt_path, attack_type):
    """åˆ†æGround Truthä¸­çš„åºåˆ—"""
    print("\n" + "=" * 80)
    print("åˆ†æ Ground Truth")
    print("=" * 80)
    
    with open(gt_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Ground Truthæ ¼å¼: { "suicide_ied": [...], "other_type": [...] }
    relevant_schemas = data.get(attack_type, [])
    
    print(f"\næ‰¾åˆ° {len(relevant_schemas)} ä¸ª {attack_type} çš„ground truthæ ·æœ¬")
    
    # æå–æ‰€æœ‰é•¿åº¦2å’Œé•¿åº¦3çš„åºåˆ—
    gt_seq2 = set()
    gt_seq3 = set()
    
    seq_length_distribution = Counter()
    
    for schema in relevant_schemas:
        events = schema.get('events', [])
        event_types = [e.get('event_type', '') for e in events if e.get('event_type', '')]
        
        seq_length_distribution[len(event_types)] += 1
        
        # æå–é•¿åº¦2çš„åºåˆ—
        for i in range(len(event_types) - 1):
            gt_seq2.add((event_types[i], event_types[i+1]))
        
        # æå–é•¿åº¦3çš„åºåˆ—
        for i in range(len(event_types) - 2):
            gt_seq3.add((event_types[i], event_types[i+1], event_types[i+2]))
    
    print(f"\nGround Truthåºåˆ—ç»Ÿè®¡:")
    print(f"  å”¯ä¸€çš„é•¿åº¦2åºåˆ—: {len(gt_seq2)} ä¸ª")
    print(f"  å”¯ä¸€çš„é•¿åº¦3åºåˆ—: {len(gt_seq3)} ä¸ª")
    
    print(f"\nGround Truthä¸­äº‹ä»¶é“¾é•¿åº¦åˆ†å¸ƒ:")
    for length, count in sorted(seq_length_distribution.items())[:10]:
        print(f"  é•¿åº¦{length}: {count}ä¸ªæ ·æœ¬")
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    print(f"\nGround Truthé•¿åº¦3åºåˆ—ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
    for i, seq in enumerate(list(gt_seq3)[:10], 1):
        print(f"  {i}. {' â†’ '.join(seq)}")
    
    return gt_seq2, gt_seq3

def compare_predictions_with_gt(graphs_dir, gt_seq2, gt_seq3):
    """å¯¹æ¯”é¢„æµ‹åºåˆ—ä¸Ground Truth"""
    print("\n" + "=" * 80)
    print("é¢„æµ‹åºåˆ— vs Ground Truth å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    # åŠ è½½æ‰€æœ‰å›¾å¹¶æå–åºåˆ—
    graph_files = []
    for filename in os.listdir(graphs_dir):
        if filename.startswith('graph_') and filename.endswith('.json') and 'merged' not in filename:
            graph_files.append(os.path.join(graphs_dir, filename))
    
    pred_seq2 = set()
    pred_seq3 = set()
    
    for file_path in graph_files:
        graph = load_graph_from_json(file_path)
        
        # æå–é•¿åº¦2åºåˆ—
        for source, target in graph.edges():
            source_attrs = graph.nodes[source]
            target_attrs = graph.nodes[target]
            
            source_type = f"{source_attrs['event_type']}.{source_attrs['event_subtype']}.{source_attrs['event_sub_subtype']}"
            target_type = f"{target_attrs['event_type']}.{target_attrs['event_subtype']}.{target_attrs['event_sub_subtype']}"
            
            pred_seq2.add((source_type, target_type))
        
        # æå–é•¿åº¦3åºåˆ—
        for node in graph.nodes():
            for succ1 in graph.successors(node):
                for succ2 in graph.successors(succ1):
                    node_attrs = graph.nodes[node]
                    succ1_attrs = graph.nodes[succ1]
                    succ2_attrs = graph.nodes[succ2]
                    
                    type1 = f"{node_attrs['event_type']}.{node_attrs['event_subtype']}.{node_attrs['event_sub_subtype']}"
                    type2 = f"{succ1_attrs['event_type']}.{succ1_attrs['event_subtype']}.{succ1_attrs['event_sub_subtype']}"
                    type3 = f"{succ2_attrs['event_type']}.{succ2_attrs['event_subtype']}.{succ2_attrs['event_sub_subtype']}"
                    
                    pred_seq3.add((type1, type2, type3))
    
    print(f"\né¢„æµ‹åºåˆ—ç»Ÿè®¡:")
    print(f"  å”¯ä¸€çš„é•¿åº¦2åºåˆ—: {len(pred_seq2)} ä¸ª")
    print(f"  å”¯ä¸€çš„é•¿åº¦3åºåˆ—: {len(pred_seq3)} ä¸ª")
    
    # è®¡ç®—äº¤é›†
    intersection_seq2 = pred_seq2 & gt_seq2
    intersection_seq3 = pred_seq3 & gt_seq3
    
    print(f"\nåŒ¹é…æƒ…å†µ:")
    print(f"  é•¿åº¦2åºåˆ—åŒ¹é…: {len(intersection_seq2)} ä¸ª")
    print(f"    - é¢„æµ‹ä¸­çš„æ­£ç¡®ç‡: {len(intersection_seq2)/len(pred_seq2)*100:.2f}% (Precision)")
    print(f"    - GTä¸­çš„è¦†ç›–ç‡: {len(intersection_seq2)/len(gt_seq2)*100:.2f}% (Recall)")
    
    print(f"  é•¿åº¦3åºåˆ—åŒ¹é…: {len(intersection_seq3)} ä¸ª")
    print(f"    - é¢„æµ‹ä¸­çš„æ­£ç¡®ç‡: {len(intersection_seq3)/len(pred_seq3)*100:.2f}% (Precision)")
    print(f"    - GTä¸­çš„è¦†ç›–ç‡: {len(intersection_seq3)/len(gt_seq3)*100:.2f}% (Recall)")
    
    # æ˜¾ç¤ºä¸€äº›åŒ¹é…å’Œä¸åŒ¹é…çš„ä¾‹å­
    print(f"\nâœ“ æˆåŠŸåŒ¹é…çš„é•¿åº¦3åºåˆ—ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
    for i, seq in enumerate(list(intersection_seq3)[:5], 1):
        print(f"  {i}. {' â†’ '.join(seq)}")
    
    print(f"\nâœ— é¢„æµ‹ä¸­ä½†GTæ²¡æœ‰çš„é•¿åº¦3åºåˆ—ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
    only_in_pred = pred_seq3 - gt_seq3
    for i, seq in enumerate(list(only_in_pred)[:5], 1):
        print(f"  {i}. {' â†’ '.join(seq)}")
    
    print(f"\nâœ— GTä¸­ä½†é¢„æµ‹ç¼ºå¤±çš„é•¿åº¦3åºåˆ—ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
    only_in_gt = gt_seq3 - pred_seq3
    for i, seq in enumerate(list(only_in_gt)[:5], 1):
        print(f"  {i}. {' â†’ '.join(seq)}")

def main():
    """ä¸»å‡½æ•°"""
    # è¯Šæ–­model2çš„å›¾
    graphs_dir = "/Users/andrewlee/Nutstore Files/æˆ‘çš„åšæœäº‘/æƒ…æŠ¥æ‚å¿—/code/result/model2/suicide_ied"
    gt_path = "/Users/andrewlee/Nutstore Files/æˆ‘çš„åšæœäº‘/æƒ…æŠ¥æ‚å¿—/code/dataset/processedData/extracted_data/event_graphs_train.json"
    attack_type = "suicide_ied"
    
    # ç¬¬ä¸€æ­¥ï¼šè¯Šæ–­å›¾ç»“æ„
    diagnose_graphs_directory(graphs_dir)
    
    # ç¬¬äºŒæ­¥ï¼šåˆ†æGround Truth
    gt_seq2, gt_seq3 = analyze_ground_truth(gt_path, attack_type)
    
    # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯”åˆ†æ
    compare_predictions_with_gt(graphs_dir, gt_seq2, gt_seq3)
    
    print("\n" + "=" * 80)
    print("ğŸ¯ å…³é”®ç»“è®º")
    print("=" * 80)
    print(f"1. å›¾ç»“æ„è´¨é‡: 85%çš„å›¾æœ‰3è·³è·¯å¾„ï¼Œå¹³å‡8.95ä¸ª/å›¾")
    print(f"2. GTåºåˆ—æ•°é‡: {len(gt_seq3)} ä¸ªå”¯ä¸€çš„é•¿åº¦3åºåˆ—")
    print(f"3. é—®é¢˜ä¸åœ¨å›¾ç»“æ„ï¼Œè€Œåœ¨äº:")
    print(f"   - äº‹ä»¶ç±»å‹åˆ†ç±»ä¸å¤Ÿå‡†ç¡®")
    print(f"   - é¢„æµ‹çš„åºåˆ—ç±»å‹ä¸GTä¸åŒ¹é…")
    print(f"   - éœ€è¦æé«˜äº‹ä»¶åˆ†ç±»çš„ç²¾åº¦")
    print("=" * 80)

if __name__ == '__main__':
    main()

