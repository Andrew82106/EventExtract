# -*- coding: utf-8 -*-
"""
åŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ„å›¾ç®—æ³• - ä¸»ç¨‹åº
æŒ‰ç…§algorithm.mdä¸­ç¬¬äºŒç§ç®—æ³•çš„æ­¥éª¤æ‰§è¡Œ
"""
import os
import sys
import shutil
import argparse
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_service import LLMService
from utils import Logger
from config import ZHIPU_API_KEY, ATTACK_TYPES, get_graphs_path, MAX_RETRIES, RETRY_DELAY, ENABLE_RESUME, FAILED_TEXTS_LOG, CACHE_ROOT
from data_loader import DataLoader
from graph_builder import GraphBuilder
from mergeGraph import GraphMerger
import json


def main(attack_type=None, api_key=None):
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒåŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ„å›¾ç®—æ³•
    
    Args:
        attack_type (str): æ”»å‡»ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„suicide_ied
        api_key (str): APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¯†é’¥
    """
    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    time_records = {}
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = Logger()
    logger.printLog("=" * 80)
    logger.printLog("å¼€å§‹æ‰§è¡ŒåŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ„å›¾ç®—æ³• (Model2)")
    logger.printLog("=" * 80)
    
    # 1. é€‰æ‹©æ”»å‡»ç±»å‹
    if attack_type is None:
        attack_type = 'suicide_ied'
    
    if attack_type not in ATTACK_TYPES:
        logger.printLog(f"è­¦å‘Š: æ”»å‡»ç±»å‹ {attack_type} ä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­")
    
    logger.printLog(f"æ­¥éª¤1: é€‰æ‹©æ”»å‡»ç±»å‹ - {attack_type}")
    
    # 2. åˆå§‹åŒ–æœåŠ¡
    step_start = time.time()
    logger.printLog("æ­¥éª¤2: åˆå§‹åŒ–æœåŠ¡...")
    
    # ä½¿ç”¨ä¼ å…¥çš„APIå¯†é’¥æˆ–é…ç½®æ–‡ä»¶ä¸­çš„å¯†é’¥
    current_api_key = api_key if api_key else ZHIPU_API_KEY
    if current_api_key == "your_api_key_here":
        logger.printLog("é”™è¯¯: è¯·åœ¨config.pyä¸­é…ç½®ZHIPU_API_KEYæˆ–é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ å…¥")
        return
    
    llm_service = LLMService(api_key=current_api_key, max_retries=MAX_RETRIES, retry_delay=RETRY_DELAY)
    data_loader = DataLoader(llm_service=llm_service)  # ä¼ é€’LLMæœåŠ¡ï¼Œç”¨äºæ™ºèƒ½é‡‡æ ·
    time_records['åˆå§‹åŒ–æœåŠ¡'] = time.time() - step_start
    logger.printLog(f"  LLMé‡è¯•é…ç½®: æœ€å¤§é‡è¯•{MAX_RETRIES}æ¬¡, å»¶è¿Ÿ{RETRY_DELAY}ç§’")
    
    # 3. åŠ è½½æ–‡æœ¬æ•°æ®
    step_start = time.time()
    logger.printLog(f"æ­¥éª¤3: åŠ è½½æ”»å‡»ç±»å‹ {attack_type} çš„æ–‡æœ¬æ•°æ®...")
    texts = data_loader.load_texts_for_attack_type(attack_type)
    time_records['åŠ è½½æ–‡æœ¬æ•°æ®'] = time.time() - step_start
    
    if not texts:
        logger.printLog("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ•°æ®")
        return
    
    logger.printLog(f"å…±åŠ è½½ {len(texts)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    # æ˜¾ç¤ºæ–‡æœ¬è¯¦æƒ…
    for i, text_data in enumerate(texts[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
        file_size = len(text_data['content'])
        logger.printLog(f"  [{i}] {os.path.basename(text_data['path'])} ({file_size} å­—ç¬¦)")
    if len(texts) > 5:
        logger.printLog(f"  ... è¿˜æœ‰ {len(texts) - 5} ä¸ªæ–‡æœ¬")
    
    # 4. å‡†å¤‡ç»“æœç›®å½•å’Œæ–­ç‚¹ç»­ä¼ 
    logger.printLog("æ­¥éª¤4: å‡†å¤‡ç»“æœç›®å½•...")
    graphs_path = get_graphs_path(attack_type)
    
    # æ£€æŸ¥å·²å¤„ç†çš„æ–‡æœ¬
    processed_texts = set()
    failed_texts_data = {}
    
    if ENABLE_RESUME and os.path.exists(graphs_path):
        logger.printLog(f"  å¯ç”¨æ–­ç‚¹ç»­ä¼ æ¨¡å¼")
        # æ‰«æå·²æœ‰çš„å›¾æ–‡ä»¶
        for filename in os.listdir(graphs_path):
            if filename.startswith('graph_') and filename.endswith('.json'):
                processed_texts.add(filename)
        logger.printLog(f"  å‘ç° {len(processed_texts)} ä¸ªå·²å¤„ç†çš„å›¾æ–‡ä»¶")
        
        # åŠ è½½å¤±è´¥è®°å½•
        if os.path.exists(FAILED_TEXTS_LOG):
            try:
                with open(FAILED_TEXTS_LOG, 'r', encoding='utf-8') as f:
                    failed_texts_data = json.load(f)
                attack_failed = failed_texts_data.get(attack_type, [])
                logger.printLog(f"  å‘ç° {len(attack_failed)} ä¸ªä¹‹å‰å¤±è´¥çš„æ–‡æœ¬è®°å½•")
            except:
                pass
    else:
        if os.path.exists(graphs_path):
            shutil.rmtree(graphs_path)
        os.makedirs(graphs_path, exist_ok=True)
        logger.printLog(f"  æ¸…ç©ºæ—§ç»“æœï¼Œé‡æ–°å¼€å§‹")
    
    os.makedirs(graphs_path, exist_ok=True)
    os.makedirs(CACHE_ROOT, exist_ok=True)
    logger.printLog(f"ç»“æœå°†ä¿å­˜åˆ°: {graphs_path}")
    
    # 5. ä¸ºæ¯ä¸ªæ–‡æœ¬æ„å»ºäº‹ä»¶å›¾ï¼ˆä½¿ç”¨åŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ–¹æ³•ï¼‰
    step_start = time.time()
    logger.printLog("æ­¥éª¤5: å¼€å§‹ä¸ºæ¯ä¸ªæ–‡æœ¬æ„å»ºäº‹ä»¶å›¾ï¼ˆåŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ–¹æ³•ï¼‰...")
    
    event_ontology = data_loader.get_event_types_description()
    event_types = data_loader.get_event_types()
    graph_builder = GraphBuilder(llm_service, event_ontology, event_types)
    
    graphs = []
    failed_texts = []
    skipped_count = 0
    
    for i, text_data in enumerate(texts, 1):
        graph_filename = f"graph_{i}.json"
        graph_path = os.path.join(graphs_path, graph_filename)
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if ENABLE_RESUME and graph_filename in processed_texts:
            # å°è¯•åŠ è½½å·²æœ‰çš„å›¾
            try:
                graph = graph_builder.load_graph_from_json(graph_path)
                if graph and graph.number_of_nodes() > 0:
                    skipped_count += 1
                    logger.printLog(f"\nâœ“ è·³è¿‡æ–‡æœ¬ {i}/{len(texts)} (å·²å¤„ç†): {os.path.basename(text_data['path'])}")
                    logger.printLog(f"  - ä»ç¼“å­˜åŠ è½½: {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {graph.number_of_edges()} æ¡è¾¹")
                    graphs.append(graph)
                    continue
                else:
                    logger.printLog(f"\n! æ–‡æœ¬ {i}/{len(texts)} å›¾æ–‡ä»¶æ— æ•ˆï¼Œå°†é‡æ–°å¤„ç†: {os.path.basename(text_data['path'])}")
            except Exception as e:
                logger.printLog(f"\n! æ–‡æœ¬ {i}/{len(texts)} åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°å¤„ç†: {os.path.basename(text_data['path'])}")
                logger.printLog(f"  é”™è¯¯: {str(e)}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œç§»é™¤processed_textsä¸­çš„è®°å½•ï¼Œç»§ç»­æ­£å¸¸å¤„ç†
            processed_texts.discard(graph_filename)
        
        logger.printLog(f"\n{'='*70}")
        logger.printLog(f"å¤„ç†æ–‡æœ¬ {i}/{len(texts)}: {os.path.basename(text_data['path'])}")
        logger.printLog(f"æ–‡æœ¬é•¿åº¦: {len(text_data['content'])} å­—ç¬¦")
        logger.printLog(f"{'='*70}")
        
        text_start_time = time.time()
        
        try:
            # æ„å»ºå›¾ï¼ˆä½¿ç”¨è¿­ä»£æ–¹æ³•ï¼‰
            graph = graph_builder.build_graph_from_text(
                text_data['content'],
                f"text_{i}"
            )
            
            text_elapsed = time.time() - text_start_time
            
            if graph and graph.number_of_nodes() > 0:
                graphs.append(graph)
                
                # ä¿å­˜å•ä¸ªå›¾
                graph_builder.save_graph_to_json(graph, graph_path)
                logger.printLog(f"âœ“ æˆåŠŸ! è€—æ—¶: {text_elapsed:.2f}ç§’")
                logger.printLog(f"  - èŠ‚ç‚¹æ•°: {graph.number_of_nodes()}")
                logger.printLog(f"  - è¾¹æ•°: {graph.number_of_edges()}")
                logger.printLog(f"  - ä¿å­˜è·¯å¾„: {graph_path}")
            else:
                logger.printLog(f"âœ— å¤±è´¥: æœªç”Ÿæˆæœ‰æ•ˆçš„å›¾ (è€—æ—¶: {text_elapsed:.2f}ç§’)")
                # è®°å½•å¤±è´¥
                failed_texts.append({
                    'index': i,
                    'path': text_data['path'],
                    'reason': 'æœªç”Ÿæˆæœ‰æ•ˆçš„å›¾'
                })
                
        except Exception as e:
            logger.printLog(f"  âœ— é”™è¯¯: {str(e)}")
            # è®°å½•å¤±è´¥
            failed_texts.append({
                'index': i,
                'path': text_data['path'],
                'reason': str(e)
            })
            import traceback
            logger.printLog(traceback.format_exc())
    
    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_texts:
        if not failed_texts_data:
            failed_texts_data = {}
        failed_texts_data[attack_type] = failed_texts
        try:
            with open(FAILED_TEXTS_LOG, 'w', encoding='utf-8') as f:
                json.dump(failed_texts_data, f, ensure_ascii=False, indent=2)
            logger.printLog(f"\nå¤±è´¥æ–‡æœ¬è®°å½•å·²ä¿å­˜åˆ°: {FAILED_TEXTS_LOG}")
        except Exception as e:
            logger.printLog(f"\nä¿å­˜å¤±è´¥è®°å½•å‡ºé”™: {str(e)}")
    
    if skipped_count > 0:
        logger.printLog(f"\næ–­ç‚¹ç»­ä¼ : è·³è¿‡äº† {skipped_count} ä¸ªå·²å¤„ç†çš„æ–‡æœ¬")
    
    logger.printLog(f"\næˆåŠŸæ„å»º {len(graphs)} ä¸ªäº‹ä»¶å›¾")
    time_records['æ„å»ºäº‹ä»¶å›¾'] = time.time() - step_start
    
    # æ‰“å°éªŒè¯æŠ¥å‘Š
    logger.printLog("\n" + "="*80)
    logger.printLog("äº‹ä»¶ç±»å‹éªŒè¯æ€»æŠ¥å‘Š")
    logger.printLog("="*80)
    graph_builder.print_validation_report()
    
    # æ‰“å°ä½ç½®ä¿¡åº¦åˆ†ç±»æŠ¥å‘Š
    logger.printLog("\n")
    graph_builder.print_low_confidence_report()
    
    # 6. èåˆæ‰€æœ‰å›¾
    step_start = time.time()
    logger.printLog("\n" + "="*80)
    logger.printLog("æ­¥éª¤6: èåˆæ‰€æœ‰äº‹ä»¶å›¾ä¸ºéª¨æ¶å›¾")
    logger.printLog("="*80)
    
    if not graphs:
        logger.printLog("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„å›¾è¿›è¡Œèåˆ")
        return
    
    logger.printLog(f"å¼€å§‹èåˆ {len(graphs)} ä¸ªäº‹ä»¶å›¾...")
    # ç»Ÿè®¡æ€»èŠ‚ç‚¹æ•°å’Œæ€»è¾¹æ•°
    total_nodes = sum(g.number_of_nodes() for g in graphs)
    total_edges = sum(g.number_of_edges() for g in graphs)
    logger.printLog(f"è¾“å…¥å›¾ç»Ÿè®¡: å…± {total_nodes} ä¸ªèŠ‚ç‚¹, {total_edges} æ¡è¾¹")
    
    graph_merger = GraphMerger()
    merged_graph = graph_merger.merge_graphs(graphs)
    
    logger.printLog(f"èåˆå®Œæˆ!")
    logger.printLog(f"  - èåˆåèŠ‚ç‚¹æ•°: {merged_graph.number_of_nodes()}")
    logger.printLog(f"  - èåˆåè¾¹æ•°: {merged_graph.number_of_edges()}")
    logger.printLog(f"  - èŠ‚ç‚¹å‹ç¼©ç‡: {(1 - merged_graph.number_of_nodes()/total_nodes)*100:.1f}%")
    
    # ä¿å­˜èåˆåçš„å›¾
    merged_graph_path = os.path.join(graphs_path, f"merged_graph_{attack_type}.json")
    graph_merger.save_merged_graph(merged_graph, merged_graph_path)
    logger.printLog(f"  - ä¿å­˜è·¯å¾„: {merged_graph_path}")
    time_records['èåˆå›¾'] = time.time() - step_start
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - total_start_time
    
    # 7. å®Œæˆ
    logger.printLog("\n" + "=" * 80)
    logger.printLog("ğŸ‰ ç®—æ³•æ‰§è¡Œå®Œæˆ!")
    logger.printLog("=" * 80)
    
    logger.printLog(f"\nã€ç®—æ³•ä¿¡æ¯ã€‘")
    logger.printLog(f"  ç®—æ³•ç±»å‹: åŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ„å›¾ç®—æ³• (Model2)")
    logger.printLog(f"  æ”»å‡»ç±»å‹: {attack_type}")
    
    logger.printLog(f"\nã€å¤„ç†ç»Ÿè®¡ã€‘")
    logger.printLog(f"  æ€»æ–‡æœ¬æ•°: {len(texts)}")
    actual_processed = len(texts) - skipped_count
    logger.printLog(f"  å®é™…å¤„ç†: {actual_processed}")
    if skipped_count > 0:
        logger.printLog(f"  è·³è¿‡ï¼ˆå·²å¤„ç†ï¼‰: {skipped_count}")
    success_count = len(graphs)
    logger.printLog(f"  æˆåŠŸæ„å»º: {success_count} ({success_count/len(texts)*100:.1f}%)")
    if failed_texts:
        logger.printLog(f"  å¤±è´¥: {len(failed_texts)} ({len(failed_texts)/len(texts)*100:.1f}%)")
    
    logger.printLog(f"\nã€å›¾ç»Ÿè®¡ã€‘")
    logger.printLog(f"  èåˆå›¾èŠ‚ç‚¹æ•°: {merged_graph.number_of_nodes()}")
    logger.printLog(f"  èåˆå›¾è¾¹æ•°: {merged_graph.number_of_edges()}")
    avg_nodes = total_nodes / len(graphs) if graphs else 0
    avg_edges = total_edges / len(graphs) if graphs else 0
    logger.printLog(f"  å¹³å‡å•å›¾èŠ‚ç‚¹æ•°: {avg_nodes:.1f}")
    logger.printLog(f"  å¹³å‡å•å›¾è¾¹æ•°: {avg_edges:.1f}")
    
    logger.printLog(f"\nã€è¾“å‡ºè·¯å¾„ã€‘")
    logger.printLog(f"  ç»“æœç›®å½•: {graphs_path}")
    logger.printLog(f"  èåˆå›¾: {merged_graph_path}")
    
    logger.printLog(f"\nã€æ—¶é—´ç»Ÿè®¡ã€‘")
    for step_name, step_time in time_records.items():
        logger.printLog(f"  {step_name}: {step_time:.2f}ç§’ ({step_time/total_time*100:.1f}%)")
    logger.printLog(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.printLog(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    
    if actual_processed > 0:
        avg_time = total_time / actual_processed
        logger.printLog(f"  å¹³å‡æ¯æ–‡æœ¬: {avg_time:.2f}ç§’")
    
    # æ‰“å°LLMè°ƒç”¨ç»Ÿè®¡
    logger.printLog(f"\nã€LLMè°ƒç”¨ç»Ÿè®¡ã€‘")
    llm_service.print_call_statistics()
    
    logger.printLog("\n" + "=" * 80)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='åŸºäºæ„ä¹‰æ®µçš„è¿­ä»£æ„å›¾ç®—æ³•')
    parser.add_argument('--attack_type', type=str, default='suicide_ied',
                        help='æ”»å‡»ç±»å‹ (é»˜è®¤: suicide_ied)')
    parser.add_argument('--api_key', type=str, default=None,
                        help='æ™ºè°±AI APIå¯†é’¥')
    
    args = parser.parse_args()
    
    main(attack_type=args.attack_type, api_key=args.api_key)

